# ML_Final_Project.ipynb To-Do List (Collaborative)

This document outlines the detailed to-do list for the monolithic Jupyter notebook, divided into three main sections for clear team member contributions and checkpoint validation.

---

**Note:**
- For each section, we will explicitly mention which course topics (by week/topic name) are covered, to ensure clarity and rubric alignment.
- Below is a checklist of all required checkpoints from the final project PDF. Each will be marked as completed in the notebook as we progress.

### Final Project PDF Checkpoints (Rubric)
- [ ] Use a real-world dataset (not toy data)
- [ ] Cover at least 10 topics from Weeks 1–12
- [ ] Submit a single, well-documented Jupyter notebook
- [ ] Include: Introduction, problem statement, dataset description
- [ ] Perform EDA, preprocessing, and feature engineering
- [ ] Implement at least 3 ML models (with at least one advanced method)
- [ ] Hyperparameter tuning and model comparison
- [ ] Evaluation metrics and learning curves
- [ ] Discussion of results, limitations, and future work
- [ ] References and appendix (if needed)
- [ ] PowerPoint slides (10–15 slides)
- [ ] Project summary report (2–3 pages)
- [ ] 5–10 minute video presentation
- [ ] Clear division of work and documentation of contributions
- [ ] Include requirements.txt and setup instructions
- [ ] Ensure academic integrity and Turnitin compliance

---

-## Section 1: Data Acquisition, EDA, and Preprocessing (Member 1)
- Topics covered: EDA, data cleaning, initial statistics (Weeks 2–4)
- [ ] Write project introduction, problem statement, and dataset description
- [ ] Load and inspect the dataset
- [ ] Perform initial exploratory data analysis (EDA)
- [ ] Visualize data distributions and correlations
- [ ] Handle missing values and outliers
- [ ] Document all findings and code

-## Section 2: Feature Engineering, Modeling, and Training (Member 2)
- Topics covered: Feature engineering, model selection, training, regularization, kernel methods, ensemble methods (Weeks 5–9)
- [ ] Engineer new features and perform feature selection
- [ ] Encode categorical variables and scale features
- [ ] Split data into training and test sets
- [ ] Implement at least three ML models (e.g., Logistic Regression, SVM, Random Forest)
- [ ] Perform hyperparameter tuning (e.g., grid search, cross-validation)
- [ ] Document modeling choices and code

-## Section 3: Evaluation, Results Analysis, and Deliverables (Member 3)
- Topics covered: Model evaluation, learning curves, model comparison, discussion, reporting (Weeks 10–12)
- [ ] Evaluate models using appropriate metrics (accuracy, precision, recall, F1, ROC-AUC)
- [ ] Plot learning curves and confusion matrices
- [ ] Compare model performance and discuss results
- [ ] Summarize findings, limitations, and future work
- [ ] Add references and appendix if needed
- [ ] Ensure notebook is clean, well-commented, and ready for submission

---

**Additional Notes:**
- Each section should start with a markdown header naming the contributor.
- Use clear code comments and markdown explanations for traceability.
- Regularly merge and review work to maintain a single, cohesive notebook.
